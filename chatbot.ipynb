{"metadata":{"language_info":{"name":"python","version":"3.7.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"pip install nltk","metadata":{"trusted":true},"execution_count":1,"outputs":[{"name":"stdout","text":"Requirement already satisfied: nltk in /srv/conda/envs/notebook/lib/python3.7/site-packages (3.5)\nRequirement already satisfied: joblib in /srv/conda/envs/notebook/lib/python3.7/site-packages (from nltk) (0.14.1)\nRequirement already satisfied: regex in /srv/conda/envs/notebook/lib/python3.7/site-packages (from nltk) (2020.5.14)\nRequirement already satisfied: tqdm in /srv/conda/envs/notebook/lib/python3.7/site-packages (from nltk) (4.46.0)\nRequirement already satisfied: click in /srv/conda/envs/notebook/lib/python3.7/site-packages (from nltk) (7.1.1)\nNote: you may need to restart the kernel to use updated packages.\n","output_type":"stream"}]},{"cell_type":"code","source":"import nltk\nimport numpy\nimport random\nimport string","metadata":{"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"code","source":"chatbots_file = open('chatbot.txt','r',errors = 'ignore')\ncontent = chatbots_file.read()\ncontent = content.lower()\nnltk.download('punkt')\nnltk.download('wordnet')\nsentence_tokens = nltk.sent_tokenize(content)\nword_tokens = nltk.word_tokenize(content)","metadata":{"trusted":true},"execution_count":3,"outputs":[{"name":"stderr","text":"[nltk_data] Downloading package punkt to /home/jovyan/nltk_data...\n[nltk_data]   Unzipping tokenizers/punkt.zip.\n[nltk_data] Downloading package wordnet to /home/jovyan/nltk_data...\n[nltk_data]   Unzipping corpora/wordnet.zip.\n","output_type":"stream"}]},{"cell_type":"code","source":"lemmer = nltk.stem.WordNetLemmatizer()","metadata":{"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"code","source":"def lem_tokens(tokens):\n    return[lemmer.lemmatize(token) for token in tokens]","metadata":{"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"code","source":"remove_punct_dict = dict((ord(punct),None) for punct in string.punctuation)","metadata":{"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"code","source":"print(string.punctuation)","metadata":{"trusted":true},"execution_count":7,"outputs":[{"name":"stdout","text":"!\"#$%&'()*+,-./:;<=>?@[\\]^_`{|}~\n","output_type":"stream"}]},{"cell_type":"code","source":"def lem_normalize(text):\n    return lem_tokens(nltk.word_tokenize(text.lower().translate(remove_punct_dict)))","metadata":{"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"code","source":"GREETING_INPUTS = (\"hello\",\"hi\",\"what's up\",\"sup\",\"hey\")\nGREETING_RESPONSES = (\"hi\",\"hey\",\"*nods*\",\"hi there\",\"hello\",\"I am glad! You are talking to me\")\n\ndef greeting(sentence):\n    for word in sentence.split(\" \"):\n        if word.lower() in GREETING_INPUTS:\n            return random.choice(GREETING_RESPONSES)","metadata":{"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"code","source":"from sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.metrics.pairwise import cosine_similarity","metadata":{"trusted":true},"execution_count":10,"outputs":[]},{"cell_type":"code","source":"def response(user_response):\n    robo_response = ''\n    sentence_tokens.append(user_response)\n    TfidfVec = TfidfVectorizer(tokenizer = lem_normalize, stop_words = 'english')\n    tfidf = TfidfVec.fit_transform(sentence_tokens)\n    \n    values = cosine_similarity(tfidf[-1],tfidf)\n    idx = values.argsort()[0][-2]\n    flat = values.flatten()\n    flat.sort()\n    \n    req_tfidf = flat[-2]\n    if(req_tfidf==0):\n        robo_response = robo_response + \"I am sorry! I don't understand you\"\n    else:\n        robo_response = robo_response + sentence_tokens[idx]\n    return robo_response","metadata":{"trusted":true},"execution_count":11,"outputs":[]},{"cell_type":"code","source":"flag = True\nprint(\"Moley: My name is Moley. I will answer your queries about chatbots. If you want to exit, type bye!\")\nwhile(flag==True):\n    user_response = input()\n    user_response = user_response.lower()\n    if(user_response!='bye'):\n        if(user_response=='thanks' or user_response=='thank you'):\n            flag = False\n            print(\"Moley: You are welcome\")\n        else:\n            if(greeting(user_response)!=None):\n                print(\"Moley: \"+greeting(user_response))\n            else:\n                print(\"Moley: \")\n                print(response(user_response))\n                sentence_tokens.remove(user_response)\n    else:\n        flag = False\n        print(\"Moley: Bye, take care! \")","metadata":{"trusted":true},"execution_count":12,"outputs":[{"name":"stdout","text":"Moley: My name is Moley. I will answer your queries about chatbots. If you want to exit, type bye!\n","output_type":"stream"},{"output_type":"stream","name":"stdin","text":" hi\n"},{"name":"stdout","text":"Moley: *nods*\n","output_type":"stream"},{"output_type":"stream","name":"stdin","text":" what is a turing machine?\n"},{"name":"stdout","text":"Moley: \n","output_type":"stream"},{"name":"stderr","text":"/srv/conda/envs/notebook/lib/python3.7/site-packages/sklearn/feature_extraction/text.py:300: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ha', 'le', 'u', 'wa'] not in stop_words.\n  'stop_words.' % sorted(inconsistent))\n","output_type":"stream"},{"name":"stdout","text":"background\nin 1950, alan turing's famous article \"computing machinery and intelligence\" was published, which proposed what is now called the turing test as a criterion of intelligence.\n","output_type":"stream"},{"output_type":"stream","name":"stdin","text":" who are the classic historic early chatbots?\n"},{"name":"stderr","text":"/srv/conda/envs/notebook/lib/python3.7/site-packages/sklearn/feature_extraction/text.py:300: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ha', 'le', 'u', 'wa'] not in stop_words.\n  'stop_words.' % sorted(inconsistent))\n","output_type":"stream"},{"name":"stdout","text":"Moley: \ndevelopment\nthe classic historic early chatbots are eliza (1966) and parry (1972).more recent notable programs include a.l.i.c.e., jabberwacky and d.u.d.e (agence nationale de la recherche and cnrs 2006).\n","output_type":"stream"},{"output_type":"stream","name":"stdin","text":" bye\n"},{"name":"stdout","text":"Moley: Bye, take care! \n","output_type":"stream"}]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}